{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script loads coffea output files and transforms the coffea histograms to a set of 1D ROOT histograms for use with the ROOT template fitting tool, `TFractionFitter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffea import hist, util\n",
    "import numpy as np\n",
    "import uproot\n",
    "\n",
    "from ttgamma.utils.plotting import RebinHist, SetRangeHist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we ran our condor jobs on some of the datasets separately, here we add together all the outputs into a single output, one each for MC and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'photon_pt': <Hist (dataset,pt,category,lepFlavor,systematic) instance at 0x7f281bebd710>,\n",
       " 'photon_eta': <Hist (dataset,eta,category,lepFlavor,systematic) instance at 0x7f281bd816d8>,\n",
       " 'photon_chIso': <Hist (dataset,chIso,category,lepFlavor,systematic) instance at 0x7f281bbd5d30>,\n",
       " 'photon_lepton_mass': <Hist (dataset,mass,category,lepFlavor,systematic) instance at 0x7f281bab2438>,\n",
       " 'photon_lepton_mass_3j0t': <Hist (dataset,mass,category,lepFlavor,systematic) instance at 0x7f281b985a90>,\n",
       " 'M3': <Hist (dataset,M3,category,lepFlavor,systematic) instance at 0x7f281b7e2080>,\n",
       " 'M3Presel': <Hist (dataset,M3,lepFlavor,systematic) instance at 0x7f281b69e6a0>,\n",
       " 'EventCount': value_accumulator(int, 152664130)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nJets = 4\n",
    "\n",
    "outputMC = util.load(f'outputMCOther_ttgamma_condorFull_{nJets}jet.coffea')\n",
    "outputMC.add(util.load(f'outputMCSingletop_ttgamma_condorFull_{nJets}jet.coffea'))\n",
    "outputMC.add(util.load(f'outputMCTTbar1l_ttgamma_condorFull_{nJets}jet.coffea'))\n",
    "outputMC.add(util.load(f'outputMCTTbar2l_ttgamma_condorFull_{nJets}jet.coffea'))\n",
    "outputMC.add(util.load(f'outputMCTTGamma_ttgamma_condorFull_{nJets}jet.coffea'))\n",
    "outputMC.add(util.load(f'outputMCWJets_ttgamma_condorFull_{nJets}jet.coffea'))\n",
    "outputMC.add(util.load(f'outputMCZJets_ttgamma_condorFull_{nJets}jet.coffea'))\n",
    "\n",
    "outputData = util.load(f'outputData_ttgamma_condorFull_{nJets}jet.coffea')\n",
    "\n",
    "outputMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the output above, we can see what histograms the processor produced.  Each histogram is multidimensional and needs reduction to a simple 1-dimension ROOT `TH1` before fitting.  We'll need to merge different datasets into the various signal and background categories, which are different depending on each fit. Also we'll handle merging the lepton flavor categories, and making the different templates for systematic variations.  Below we enumerate the MC datasets that were processed in making the `M3` histogram.  You can switch out the axis name for any of the other axes in the histogram (each name listed in `<Hist (dataset,M3,category,lepFlavor,systematic) instance at ...>`) to see what bins are filled along each axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<StringBin (DYjetsM10to50) instance at 0x7f281cf62e48>,\n",
       " <StringBin (DYjetsM50) instance at 0x7f2850da8ba8>,\n",
       " <StringBin (GJets_HT100To200) instance at 0x7f281bec21d0>,\n",
       " <StringBin (GJets_HT200To400) instance at 0x7f281bec2128>,\n",
       " <StringBin (GJets_HT400To600) instance at 0x7f281bec2160>,\n",
       " <StringBin (GJets_HT40To100) instance at 0x7f281bec2208>,\n",
       " <StringBin (GJets_HT600ToInf) instance at 0x7f281bec2198>,\n",
       " <StringBin (QCD_Pt1000toInf_Mu) instance at 0x7f281bebdcf8>,\n",
       " <StringBin (QCD_Pt120to170_Ele) instance at 0x7f281bec2048>,\n",
       " <StringBin (QCD_Pt120to170_Mu) instance at 0x7f281bebde48>,\n",
       " <StringBin (QCD_Pt170to300_Ele) instance at 0x7f281bebdf60>,\n",
       " <StringBin (QCD_Pt170to300_Mu) instance at 0x7f281bebddd8>,\n",
       " <StringBin (QCD_Pt20to30_Ele) instance at 0x7f281bec20f0>,\n",
       " <StringBin (QCD_Pt20to30_Mu) instance at 0x7f281bebdeb8>,\n",
       " <StringBin (QCD_Pt300to470_Mu) instance at 0x7f281bebde10>,\n",
       " <StringBin (QCD_Pt300toInf_Ele) instance at 0x7f281bebdf98>,\n",
       " <StringBin (QCD_Pt30to50_Ele) instance at 0x7f281bec20b8>,\n",
       " <StringBin (QCD_Pt30to50_Mu) instance at 0x7f281bebdf28>,\n",
       " <StringBin (QCD_Pt470to600_Mu) instance at 0x7f281bebdda0>,\n",
       " <StringBin (QCD_Pt50to80_Ele) instance at 0x7f281bec2080>,\n",
       " <StringBin (QCD_Pt50to80_Mu) instance at 0x7f281bebdef0>,\n",
       " <StringBin (QCD_Pt600to800_Mu) instance at 0x7f281bebdd68>,\n",
       " <StringBin (QCD_Pt800to1000_Mu) instance at 0x7f281bebdd30>,\n",
       " <StringBin (QCD_Pt80to120_Ele) instance at 0x7f281bebdfd0>,\n",
       " <StringBin (QCD_Pt80to120_Mu) instance at 0x7f281bebde80>,\n",
       " <StringBin (ST_s_channel) instance at 0x7f281bec2240>,\n",
       " <StringBin (ST_tW_channel) instance at 0x7f281c8b8b00>,\n",
       " <StringBin (ST_t_channel) instance at 0x7f2838f58e80>,\n",
       " <StringBin (ST_tbarW_channel) instance at 0x7f281dc79748>,\n",
       " <StringBin (ST_tbar_channel) instance at 0x7f281dc79780>,\n",
       " <StringBin (TTGamma_Dilepton) instance at 0x7f281c8d7160>,\n",
       " <StringBin (TTGamma_Hadronic) instance at 0x7f281b6a3e10>,\n",
       " <StringBin (TTGamma_SingleLept) instance at 0x7f2850db6278>,\n",
       " <StringBin (TTWtoLNu) instance at 0x7f281bebdcc0>,\n",
       " <StringBin (TTWtoQQ) instance at 0x7f281bebdc88>,\n",
       " <StringBin (TTZtoLL) instance at 0x7f281bebdc18>,\n",
       " <StringBin (TTbarPowheg_Dilepton) instance at 0x7f2850db63c8>,\n",
       " <StringBin (TTbarPowheg_Hadronic) instance at 0x7f281ddde710>,\n",
       " <StringBin (TTbarPowheg_Semilept) instance at 0x7f281bec2e10>,\n",
       " <StringBin (W1jets) instance at 0x7f281bec2f28>,\n",
       " <StringBin (W2jets) instance at 0x7f281c8b8a20>,\n",
       " <StringBin (W3jets) instance at 0x7f281dc90208>,\n",
       " <StringBin (W4jets) instance at 0x7f281cf62d30>,\n",
       " <StringBin (WGamma_01J_5f) instance at 0x7f281bebdba8>,\n",
       " <StringBin (WW) instance at 0x7f281bebdb38>,\n",
       " <StringBin (WZ) instance at 0x7f281bebda58>,\n",
       " <StringBin (ZGamma_01J_5f_lowMass) instance at 0x7f281bebdb70>,\n",
       " <StringBin (ZZ) instance at 0x7f281bebdac8>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputMC['M3'].identifiers('dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we first sum up all lepton flavors and photon gen categories for MC, then we group the M3 distributions into $t\\bar{t}$ and non-$t\\bar{t}$ categories, then we rebin the `M3` variable to be slightly coarser (merging every 5 bins), leaving a 3D histogram with `dataset,M3,systematic` axes.  For data, we only have to rebin the `M3` axis, and sum the rest (since each only has one entry).  Then we open a new ROOT output file and loop through the dataset and systematic axes, saving a 1D projection histogram for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms_data/d3/ncsmith/cmsdas2020/TTGamma_LongExercise/coffeaenv/lib/python3.6/site-packages/coffea/hist/hist_tools.py:346: RuntimeWarning: Not all requested indices present in <Cat (name=dataset) instance at 0x7f281bebd908>\n",
      "  warnings.warn(\"Not all requested indices present in %r\" % self, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "h = outputMC['M3'].sum('lepFlavor', 'category')\n",
    "\n",
    "groupingTop = {'TopPair': ['TTGamma_Dilepton','TTGamma_SingleLept','TTGamma_Hadronic',\n",
    "                           'TTbarPowheg_Dilepton', 'TTbarPowheg_Semilept', 'TTbarPowheg_Hadronic'],\n",
    "               'NonTop' : ['W1jets', 'W2jets', 'W3jets', 'W4jets',\n",
    "                           'DYjetsM10to50', 'DYjetsM50'\n",
    "                           'ST_s_channel', 'ST_tW_channel', 'ST_tbarW_channel', 'ST_tbar_channel', 'ST_t_channel',\n",
    "                           'WGamma_01J_5f',\n",
    "                           'ZGamma_01J_5f_lowMass',\n",
    "                           'TTWtoLNu','TTWtoQQ','TTZtoLL',\n",
    "                           'GJets_HT40To100', 'GJets_HT100To200', 'GJets_HT200To400', 'GJets_HT400To600', 'GJets_HT600ToInf', \n",
    "                           'QCD_Pt20to30_Ele', 'QCD_Pt30to50_Ele', 'QCD_Pt50to80_Ele', 'QCD_Pt80to120_Ele', 'QCD_Pt120to170_Ele', 'QCD_Pt170to300_Ele', 'QCD_Pt300toInf_Ele', 'QCD_Pt20to30_Mu', 'QCD_Pt30to50_Mu', 'QCD_Pt50to80_Mu', 'QCD_Pt80to120_Mu', 'QCD_Pt120to170_Mu', 'QCD_Pt170to300_Mu', 'QCD_Pt300to470_Mu', 'QCD_Pt470to600_Mu', 'QCD_Pt600to800_Mu', 'QCD_Pt800to1000_Mu', 'QCD_Pt1000toInf_Mu'\n",
    "                          ],\n",
    "              }\n",
    "h = h.group('dataset', hist.Cat('dataset', 'Samples', sorting='placement'), groupingTop)\n",
    "\n",
    "h = RebinHist(h, \"M3\", 5)\n",
    "h = h[:, 50.:550.]  # first axis is dataset, second is the M3 numeric range\n",
    "\n",
    "hData = outputData['M3']\n",
    "hData = hData.sum('lepFlavor', 'dataset', 'category', 'systematic')\n",
    "hData = RebinHist(hData, \"M3\", 5)\n",
    "hData = hData[50.:550.]\n",
    "\n",
    "!mkdir -p RootFiles\n",
    "outputFile = uproot.recreate(\"RootFiles/M3_Output.root\")\n",
    "\n",
    "outputFile['dataObs'] = hist.export1d(hData)\n",
    "\n",
    "datasets = h.axis('dataset').identifiers()\n",
    "systematics = h.axis('systematic').identifiers()\n",
    "for _dataset in datasets:\n",
    "    for _systematic in systematics:\n",
    "        outputFile[f'{_dataset}_{_systematic}'] = hist.export1d(h.integrate('dataset',_dataset).integrate('systematic',_systematic))\n",
    "\n",
    "outputFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we get the photon charged hadron isolation histogram, and sum all lepton flavors and datasets (since we don't care here what dataset the photon came from) and then group them into isolated and nonprompt categories based on the gen-matching `category` axis.  Lastly, we rebin the `chIso` axis to be slightly coarser to help guard against low statistics when fitting, leaving a 3D histogram with `category,M3,systematic` axes.  We are going to fit the data to the sum of the categories, so we simply sum all axes except `chIso` for the data and rebin it to match the MC binning.  Then, as before, we save a 1D projection for each category and systematic combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = outputMC['photon_chIso'].sum('lepFlavor', 'dataset')\n",
    "\n",
    "groupingPho= {\"Isolated\": slice(1,3),\n",
    "              \"NonPrompt\":slice(3,5),\n",
    "             }\n",
    "h = h.group('category', hist.Cat('category', 'Samples', sorting='placement'), groupingPho)\n",
    "\n",
    "chIso_newbins = np.array([0,1.141,2.5,5,10,15,20])\n",
    "h = h.rebin(\"chIso\", hist.Bin(\"chIso\", h.axis(\"chIso\").label, chIso_newbins))\n",
    "\n",
    "hData = outputData['photon_chIso'].sum('lepFlavor', 'dataset')\n",
    "hData = hData.sum('category')\n",
    "hData = hData.sum('systematic')\n",
    "hData = hData.rebin(\"chIso\", hist.Bin(\"chIso\", hData.axis(\"chIso\").label, chIso_newbins))\n",
    "\n",
    "outputFile = uproot.recreate(\"RootFiles/Isolation_Output.root\")\n",
    "outputFile['dataObs'] = hist.export1d(hData)\n",
    "\n",
    "categories = h.axis('category').identifiers()\n",
    "systematics = h.axis('systematic').identifiers()\n",
    "for _category in categories:\n",
    "    for _systematic in systematics:\n",
    "        outputFile[f'{_category}_{_systematic}'] = hist.export1d(h.integrate('category',_category).integrate('systematic',_systematic))\n",
    "    \n",
    "outputFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the $e\\gamma$ mass histograms, we regroup the gen-matching photon category into just three categories: genuine, mis-ID electrons, and non-prompt photons, then regroup the different datasets into signal and background samples, then rebin the mass to be slightly coarser and restrict the range.  Then we create 1D projections for each flavor, sample, and systematic combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = outputMC['photon_lepton_mass_3j0t']\n",
    "\n",
    "groupingPho= {\"Genuine\":slice(1,2),\n",
    "              \"MisIDele\": slice(2,3),\n",
    "              \"NonPrompt\":slice(3,5),\n",
    "             }\n",
    "h = h.group('category', hist.Cat('category', 'Samples', sorting='placement'), groupingPho)\n",
    "\n",
    "\n",
    "groupingDataset = {'WGamma' : ['WGamma_01J_5f'],\n",
    "                   \"ZGamma\" : ['ZGamma_01J_5f_lowMass'],\n",
    "                   \"Other\"  : ['TTGamma_Dilepton','TTGamma_SingleLept','TTGamma_Hadronic',\n",
    "                               'TTbarPowheg_Dilepton', 'TTbarPowheg_Semilept', 'TTbarPowheg_Hadronic',\n",
    "                               'W1jets', 'W2jets', 'W3jets', 'W4jets',\n",
    "                               'DYjetsM50', 'DYjetsM10to50',\n",
    "                               'ST_s_channel', 'ST_tW_channel', 'ST_tbarW_channel', 'ST_tbar_channel', 'ST_t_channel',\n",
    "                               'TTWtoLNu','TTWtoQQ','TTZtoLL',\n",
    "                              ],\n",
    "                  }\n",
    "h = h.group('dataset', hist.Cat('dataset', 'Samples', sorting='placement'), groupingDataset)\n",
    "\n",
    "h = RebinHist(h,\"mass\",5)[:, :, 40.:200.]  # mass is the 3rd axis\n",
    "\n",
    "hData = outputData['photon_lepton_mass_3j0t']\n",
    "hData = hData.sum('dataset')\n",
    "hData = hData.sum('category')\n",
    "hData = hData.sum('systematic')\n",
    "hData = RebinHist(hData,\"mass\",5)\n",
    "hData = SetRangeHist(hData,\"mass\",40,200)\n",
    "\n",
    "\n",
    "systematics = h.axis('systematic').identifiers()\n",
    "\n",
    "for _lepton in ['electron', 'muon']:\n",
    "    outputFile = uproot.recreate(f\"RootFiles/MisID_Output_{_lepton}.root\")\n",
    "\n",
    "    outputFile[\"dataObs\"] = hist.export1d(hData.integrate(\"lepFlavor\",_lepton))\n",
    "\n",
    "    hMisID = h.integrate(\"category\",\"MisIDele\").sum(\"dataset\").integrate(\"lepFlavor\",_lepton)\n",
    "    hOther = h.integrate(\"category\",[\"Genuine\",\"NonPrompt\"]).integrate(\"lepFlavor\",_lepton)\n",
    "    datasets = hOther.axis('dataset').identifiers()\n",
    "\n",
    "    for _systematic in systematics:\n",
    "        outputFile[f'MisIDele_{_systematic}'] = hist.export1d(hMisID.integrate('systematic',_systematic))\n",
    "        for _dataset in datasets:\n",
    "            outputFile[f'{_dataset}_{_systematic}'] =  hist.export1d(hOther.integrate(\"dataset\",_dataset).integrate('systematic',_systematic))\n",
    "\n",
    "    \n",
    "    outputFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coffeaenv",
   "language": "python",
   "name": "coffeaenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
